import torch
import torch.nn as nn
import numpy as np
from torchvision import models, transforms, datasets
from tqdm import tqdm
import os
import sys
import matplotlib
matplotlib.use('Agg')
from matplotlib import pyplot as plt
from matplotlib.patches import Ellipse
from torch.utils.data import TensorDataset, DataLoader
import shutil
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import math

'''
ä¼ å‚æ€è·¯:

    main[ä¸»å‡½æ•°] -->|åˆå§‹åŒ–| universal_perturbation[å…¨å±€æ‰°åŠ¨]
    main -->|é€ç±»åˆ«å¾ªç¯| attack[æ”»å‡»å®ä¾‹]
    attack -->|ä¼ å…¥| universal_perturbation
    attack -->|æ›´æ–°| universal_perturbation
    evaluate_global_accuracy -->|åº”ç”¨æ‰°åŠ¨| universal_perturbation
'''
# --- å…¨å±€è¶…å‚æ•° ---
epsilon = 0.25       # æ€»æ‰°åŠ¨é¢„ç®—
# å—é€‰æ‹©ç­–ç•¥
block_size = 8     # å—å°ºå¯¸
num_samples = 16     # æ¯è½®é‡‡æ ·å—æ•°
block_mu = 1         # å—é€‰æ‹©ç­–ç•¥ä¸­æˆåŠŸå—çš„ä¸ªæ•°(è€å¸ˆçš„è¦æ±‚è¿™ä¸ªæœ€ç»ˆå¯èƒ½è¦è®¾ç½®æˆ1 ?)
max_iter = 10      # å¢åŠ è¿­ä»£æ¬¡æ•°ä»¥è§‚å¯Ÿè¶‹åŠ¿,è¿™ä¸ªä¸åº”è¯¥è®¾çš„å¤ªå°(æ¯”å¦‚3)
num_test_classes = 200  # æµ‹è¯•ç±»åˆ«æ•°ï¼ˆä»200ç±»ä¸­éšæœºé€‰æ‹©ï¼‰
target_class = None # å½“å‰æ˜¯æ— ç›®æ ‡æ”»å‡» 
lambda_sparsity = 0.3  # ç¨€ç–æ€§æ­£åˆ™åŒ–ç³»æ•°

debug_mode = 0  # 1å¯ç”¨æ—¥å¿—å’ŒæŠ¥å‘Šï¼Œ0ç¦ç”¨ï¼ˆè®¾ç½®ä¸º0æ—¶æ‰§è¡ŒåŸç‰ˆé€»è¾‘ï¼‰

# --- CMA-ES å‚æ•° ---


# cma_lambda = 10     # ç§ç¾¤å¤§å°[è¯¥å‚æ•°åœ¨å½“å‰ç‰ˆæœ¬ä»£ç ä¸­æœªä½¿ç”¨,ä»£ç ä¸­ç”¨çš„æ˜¯4 + 3ln(n)]
# cma_mu = 5          # çˆ¶ä»£æ•°é‡[è¯¥å‚æ•°æœªä½¿ç”¨,ä»£ç ä¸­ä½¿ç”¨å’Œnæœ‰å…³çš„å‡½æ•°]
cma_sigma = 0.2     # åˆå§‹æ­¥é•¿
img_size = 64 #tiny-imagenetçš„å¤§å°æ˜¯64
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(img_size),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ToTensor(),
    transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2770, 0.2691, 0.2821])
])

val_transform = transforms.Compose([
    transforms.Resize(img_size),
    transforms.CenterCrop(img_size),
    transforms.ToTensor(),
    transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2770, 0.2691, 0.2821])
]) #è¿™é‡Œå·²ç»è¿›è¡Œå½’ä¸€åŒ–äº†ï¼Œåªèƒ½æœ‰è¿™ä¸€æ¬¡ï¼Œåé¢éƒ½ä¸è®¸å½’ä¸€åŒ–ï¼
# åœ¨ evaluate å’Œ attack ç­‰å‡½æ•°ä¸­ä¸éœ€è¦å†æ¬¡è°ƒç”¨ normalize



# éšæœºæ€§çš„é™å®š--ä½¿å¾—æ¨¡å‹å‡†ç¡®ç‡åœ¨æ¯æ¬¡è¿è¡Œçš„è¿‡ç¨‹ä¸­æ›´ç¨³å®š,ä¾¿äºåˆ†æ
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True  # ç¡®ä¿å·ç§¯æ“ä½œç¡®å®šæ€§
    torch.backends.cudnn.benchmark = False     # å…³é—­è‡ªåŠ¨ä¼˜åŒ–(é¿å…å¼•å…¥éšæœºæ€§)
# é€šè¿‡ä¸€ä¸ªå‚æ•°çš„è°ƒæ•´ï¼Œå¯ä»¥åŠ è½½1ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥æ˜¯æŸä¸ªç™¾åˆ†æ¯”çš„æ ·æœ¬ï¼ˆæ¯”å¦‚10%ã€20%ç­‰ï¼‰
def get_tinyimagenet_loader(data_dir, batch_size=64, train=False):
    """åŠ è½½Tiny ImageNetæ•°æ®é›†ï¼ˆæ¯ä¸ªç±»åˆ«10%æ•°æ®ï¼‰"""
    transform = val_transform if not train else train_transform
    
    dataset_path = os.path.join(data_dir, 'train' if train else 'val')
    
    # æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹
    checkpoint_path = os.path.join(dataset_path, '.ipynb_checkpoints')
    if os.path.exists(checkpoint_path):
        shutil.rmtree(checkpoint_path)
    
    # åŠ è½½å®Œæ•´æ•°æ®é›†
    full_dataset = datasets.ImageFolder(dataset_path, transform=transform)
    
    # æŒ‰ç±»åˆ«åˆ†å±‚é‡‡æ ·10%
    from collections import defaultdict
    import random
    random.seed(42)  # å›ºå®šéšæœºç§å­ä¿è¯å¯é‡å¤æ€§
    
    # åˆ›å»ºç±»åˆ«->ç´¢å¼•çš„æ˜ å°„
    class_indices = defaultdict(list)
    for idx, (_, label) in enumerate(full_dataset.samples):
        class_indices[label].append(idx)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«é‡‡æ ·æ•°é‡ï¼ˆè‡³å°‘1ä¸ªï¼‰
    sampled_indices = []
    for label, indices in class_indices.items():
        n_total = len(indices)
        n_samples = max(1, int(0.1 * n_total))  # å…³é”®ä¿®æ”¹ç‚¹ï¼š10%é‡‡æ ·
        sampled_indices.extend(random.sample(indices, n_samples))
    
    # åˆ›å»ºé‡‡æ ·åçš„å­é›†
    sampled_dataset = torch.utils.data.Subset(full_dataset, sampled_indices)
    
    return DataLoader(
        sampled_dataset,
        batch_size=batch_size,
        shuffle=False,         # ä¿æŒåŸå§‹shuffleå‚æ•°
        num_workers=4,
        pin_memory=True
    )

# æ–°å¢Thompson Samplingæ¦‚ç‡æ›´æ–°ç±»ï¼Œè¿™ä¸ªåœ¨SZOAttackä¸­ä¼šåŒ–æˆä¸€ä¸ªts_updaterå¯¹è±¡
class ThompsonSamplingUpdater:
    def __init__(self, total_blocks, device):
        self.alpha = torch.ones(total_blocks, device=device) + 1e-6  # æˆåŠŸè®¡æ•°
        self.beta = torch.ones(total_blocks, device=device)   # å¤±è´¥è®¡æ•°
        self.total_blocks = total_blocks
        self.device = device

    def update(self, selected_blocks, successes): #æ›´æ–°å—å†…æ‰°åŠ¨
        """æ›´æ–°Betaåˆ†å¸ƒå‚æ•°"""
        for bid in selected_blocks.unique():
            mask = (selected_blocks == bid)
            success_count = successes[mask].sum().item()
            fail_count = mask.sum().item() - success_count
            
            self.alpha[bid] += success_count
            self.beta[bid] += fail_count

    def sample_probs(self):
        """ä»Betaåˆ†å¸ƒé‡‡æ ·æ¦‚ç‡"""
        return torch.distributions.Beta(self.alpha, self.beta).sample()


class DebugVGG16(nn.Module):
    """åŠ è½½æœ¬åœ°é¢„è®­ç»ƒçš„VGG16æ¨¡å‹"""
    def __init__(self, num_classes=200):
        super().__init__()
        # åˆå§‹åŒ–æ ‡å‡†VGG16ç»“æ„ï¼ˆä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼‰
        # æˆ‘å°è¯•è¿‡,è¿™é‡ŒåŠ è½½weights=models.VGG16_Weights.IMAGENET1K_V1ä¸ä¼šæœ‰æ€§èƒ½æå‡,å’ŒNoneæ˜¯ä¸€æ ·çš„
        original_model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1) 

        # ç‰¹å¾æå–å±‚ä¿æŒä¸å˜
        self.features = original_model.features
        
        # åŠ¨æ€è®¡ç®—åˆ†ç±»å™¨è¾“å…¥ç»´åº¦ï¼ˆé€‚é…64x64è¾“å…¥ï¼‰
        with torch.no_grad():
            dummy = torch.randn(1, 3, 64, 64)
            features = self.features(dummy)
            # in_features = features.view(1, -1).size(1) # è¿™è¡Œå’Œä¸‹é¢é‚£è¡Œåªèƒ½ç•™ä¸‹ä¸€è¡Œ
            in_features = features.view(-1).shape[0] # è¿™è¡Œæ˜¯ç›´æ¥ä»train.pyä¸­æŠ„è¿‡æ¥çš„

        # é‡å»ºåˆ†ç±»å™¨å±‚
        self.classifier = nn.Sequential(
            nn.Linear(in_features, 4096),
            nn.ReLU(True),
            nn.Dropout(0.5), #è¿™ä¸ªæ”¾åœ¨åˆ«å¤„å¯èƒ½ä¹Ÿä¼šå½±å“æ€§èƒ½ï¼Œä½†æ˜¯è¿™é‡Œå’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ã€‚æˆ‘è§‰å¾—å¯èƒ½å½±å“ä¸å¤§
            nn.Linear(4096, 4096),
            nn.ReLU(True),
            nn.Dropout(0.5), #åŒä¸Š
            nn.Linear(4096, num_classes)
        )

        # åŠ è½½æœ¬åœ°é¢„è®­ç»ƒæƒé‡
        pretrained_dict = torch.load("../train_vgg16/best_vgg16_1.pth", map_location="cuda:0")
        self.load_state_dict(pretrained_dict)
        # åŠ è½½æƒé‡åï¼Œæ‰“å°ç¼ºå¤±å’Œæ„å¤–çš„é”®
        missing, unexpected = self.load_state_dict(pretrained_dict, strict=False)
        print("ç¼ºå¤±çš„é”®ï¼ˆæœªåŠ è½½çš„æƒé‡ï¼‰:", missing)
        print("æ„å¤–çš„é”®ï¼ˆå¤šä½™çš„æƒé‡ï¼‰:", unexpected)
        print("æˆåŠŸåŠ è½½æœ¬åœ°VGG16æƒé‡")



    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

# ---------------------------- CMA-ESä¼˜åŒ–å™¨ ---------------------------
class CMAESBlock:
    """ç®¡ç†æ¯ä¸ªå—çš„CMA-ESä¼˜åŒ–å™¨"""
    def __init__(self, block_size):
        self.dim = block_size**2 * 3  # è¿™ä¸ªå¥½åƒæ˜¯å—ä¸­çš„åƒç´ ç‚¹ä¸ªæ•°ï¼Œä¸€ä¼šæ‰“å°ä¸€ä¸‹ï¼Ÿ
        self.best_fitness = -np.inf  # æ–°å¢ï¼šè·Ÿè¸ªå—çš„æœ€ä½³é€‚åº”åº¦
        # åŠ¨æ€è®¡ç®— lambda å’Œ mu
        self.lambda_ = int(4 + 3 * math.log(self.dim))  # å…¬å¼ Î» = 4 + 3*ln(n) pdfå…¬å¼(48)
        self.mu = max(1, self.lambda_ // 2)             # Î¼ = Î»/2ï¼ˆ"//"æ˜¯æ•´é™¤çš„æ„æ€ï¼Œå‘ä¸‹å–æ•´ï¼Œè‡³å°‘ä¸º1ï¼‰,
        # self.lambda_ = cma_lambda  # ä¸å†åŠ¨æ€è®¡ç®—
        # self.mu = cma_mu           # ç›´æ¥ä½¿ç”¨å…¨å±€å‚æ•°
        self.mean = np.zeros(self.dim)
        self.C = np.eye(self.dim)  # åæ–¹å·®çŸ©é˜µ;åˆå§‹åŒ–ä¸ºå•ä½çŸ©é˜µ
        self.sigma = cma_sigma #è¿™ä¸ªæ˜¯ç¡¬è®¾ç½®çš„ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´
        self.p_sigma = np.zeros(self.dim)
        self.p_c = np.zeros(self.dim)

        # æƒé‡è®¡ç®—ï¼ˆPDFå…¬å¼49-53ï¼‰
        raw_weights = [math.log((self.lambda_ + 1)/2) - math.log(i+1) for i in range(self.lambda_)]
        positive_weights = raw_weights[:self.mu]
        negative_weights = raw_weights[self.mu:]

        sum_pos = sum(abs(w) for w in positive_weights)
        sum_neg = sum(abs(w) for w in negative_weights)
        positive_weights = [w/sum_pos for w in positive_weights]

        # è®¡ç®—Î¼_effï¼ˆPDFç¬¦å·è¯´æ˜ç¬¬8æ®µï¼‰
        self.mueff = (sum(positive_weights)**2) / sum(w**2 for w in positive_weights)

        # è®¡ç®—è´Ÿæƒé‡ç¼©æ”¾å› å­ï¼ˆPDFå…¬å¼50-53ï¼‰
        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)  # ä¸´æ—¶å€¼ï¼Œåç»­ä¿®æ­£
        self.cmu = 0.25  # ä¸´æ—¶å€¼
        alpha_mu_neg = 1 + self.c1 / self.cmu
        mueff_neg = (sum(negative_weights)**2) / sum(w**2 for w in negative_weights)
        alpha_mueff_neg = 1 + 2 * mueff_neg / (self.mueff + 2)
        alpha_posdef_neg = (1 - self.c1 - self.cmu) / (self.dim * self.cmu)
        scale_neg = min(alpha_mu_neg, alpha_mueff_neg, alpha_posdef_neg) / sum_neg
        negative_weights = [w * scale_neg for w in negative_weights]

        self.weights = np.array(positive_weights + negative_weights)
        # self.mu = cma_mu
        # self.lambda_ = cma_lambda #å…ˆæŠŠè¿™é‡Œæ³¨é‡Šæ‰ï¼Œä¸é‡‡ç”¨äººä¸ºè®¾ç½®çš„ç¡¬å‚æ•°
        # å‚æ•°è®¡ç®—ï¼ˆPDFå…¬å¼55-58ï¼‰
        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)  # å…¬å¼56
        alpha_cov = 2
        temp_cmu = alpha_cov * (0.25 + self.mueff + 1/self.mueff - 2) / ((self.dim + 2)**2 + alpha_cov * self.mueff / 2)
        # self.c1 = min(1 - self.cmu, temp_cmu)
        self.cmu = min(1 - self.c1, temp_cmu)
        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)  # å…¬å¼55
        self.damps = 1 + 2*max(0, np.sqrt((self.mueff-1)/(self.dim+1)) -1) + self.cs
                # æ–°å¢å‚æ•°å†å²è®°å½•
        self.c1_history = []
        self.cmu_history = []
        self.p_sigma_history = []
        self.p_c_history = []
        self.C_history = []  # åæ–¹å·®çŸ©é˜µå†å²
        self.sigma_history = []     # è®°å½•sigmaçš„æ¼”åŒ–

        # æ–°å¢æƒé‡å®šä¹‰
        '''
        ç”¨æˆ·çš„ä»£ç ä¸­self.weightsæ˜¯å–å‰muä¸ªï¼Œç”¨np.log(mu + 0.5) - np.log(i+1)ï¼Œç„¶åå½’ä¸€åŒ–ã€‚ä½†PDFä¸­
        çš„å…¬å¼49æ˜¯w'_i = ln((Î»+1)/2) - ln(i)ï¼Œå¯¹äºi=1åˆ°Î»ã€‚ç”¨æˆ·è¿™é‡Œç”¨çš„æ˜¯muè€Œä¸æ˜¯Î»ï¼Œè¿™å¯èƒ½æœ‰é—®é¢˜ã€‚
        å› ä¸ºPDFä¸­çš„æƒé‡æ˜¯é’ˆå¯¹æ•´ä¸ªÎ»ä¸ªæ ·æœ¬çš„ï¼Œè€Œç”¨æˆ·åªå–äº†å‰muä¸ªï¼Œè¿™æ˜¾ç„¶ä¸å¯¹ã€‚æ­£ç¡®çš„åšæ³•åº”è¯¥æ˜¯ä¸ºæ‰€æœ‰Î»
        æ ·æœ¬ç”Ÿæˆæƒé‡ï¼Œç„¶åæ ¹æ®æ­£è´Ÿè¿›è¡Œè°ƒæ•´ï¼Œæ¯”å¦‚å…¬å¼49-53ã€‚ç”¨æˆ·çš„ä»£ç ä¸­åªç”Ÿæˆäº†muä¸ªæ­£æƒé‡ï¼Œè€ŒPDFä¸­å¯
        èƒ½è¿˜æœ‰è´Ÿæƒé‡ï¼Œæ‰€ä»¥è¿™é‡Œæ˜æ˜¾æœ‰è¯¯ã€‚
        '''


        #æ³¨æ„ï¼Œpdfä¸­ç‰¹åˆ«æç¤º53å¼(æƒé‡è®¾ç½®)ä¸€å®šä¸èƒ½éšä¾¿æ”¹ï¼Œä½†æ˜¯æˆ‘æ€ä¹ˆè§‰å¾—æˆ‘çš„ä»£ç ä¸­çš„è®¾ç½®å’Œpdfä¸ä¸€è‡´ï¼Ÿ
        # å†å²è®°å½•ï¼Œåé¢ç”»CMA-ESå›¾æœ‰ç”¨
        self.chi_n = np.sqrt(self.dim) * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))


        #è¿™æ˜¯ä¸ºäº†å—å†…ç¨€ç–(è¿›è€Œä½“ç°ä¸ºæ•´ä½“ç¨€ç–)è€Œå¼•å…¥çš„
        # æ–°å¢ç¨€ç–æ€§å‚æ•°
        self.sparsity = 0.1             # åˆå§‹ç¨€ç–åº¦ï¼ˆæ‰°åŠ¨å‰10%çš„åƒç´ ï¼‰
        self.active_pixels = []         # è®°å½•å½“å‰æ¿€æ´»çš„åƒç´ ç´¢å¼•

    def sample(self):
        self.samples = [
            self.mean + self.sigma * np.random.multivariate_normal(np.zeros(self.dim), self.C)
            for _ in range(self.lambda_)
        ]
        return self.samples
    def update(self, all_samples, fitness_values): #ä¼ è¿›æ¥çš„æ—¶å€™ï¼Œall_samplesæ˜¯listï¼Œfitness_valuesæ˜¯æ‰€æœ‰æ ·æœ¬çš„é€‚åº”åº¦np.arrayã€‚è¿™ä¸ªå‡½æ•°çš„bugåº”è¯¥æ˜¯è°ƒå®Œäº†
        """ä¿®æ­£åçš„æ›´æ–°å‡½æ•°"""
        # === è¾“å…¥é¢„å¤„ç† ===
        all_samples = np.array(all_samples)
        fitness_values = np.array(fitness_values)
        
        # === åŠ¨æ€è°ƒæ•´æœ‰æ•ˆæ ·æœ¬æ•° ===
        valid_samples = []
        valid_fitness = []
        for s, f in zip(all_samples, fitness_values):
            if not np.isnan(f) and s.shape == (self.dim,):
                valid_samples.append(s)
                valid_fitness.append(f)
        if len(valid_samples) < 1:
            return  # æ— æœ‰æ•ˆæ ·æœ¬æ—¶è·³è¿‡æ›´æ–°
        
        # === åŠ¨æ€è®¡ç®—å®é™…muå€¼ ===
        actual_mu = min(self.mu, len(valid_samples))
        # print("actual_mu:", actual_mu)
        sorted_indices = np.argsort(valid_fitness)[::-1][:actual_mu]  # å…³é”®ä¿®å¤ï¼šé™åˆ¶ç´¢å¼•èŒƒå›´
        
        # === æƒé‡å½’ä¸€åŒ– ===
        actual_weights = self.weights[:actual_mu]  # æˆªå–æœ‰æ•ˆæƒé‡
        actual_weights /= np.sum(actual_weights)   # é‡æ–°å½’ä¸€åŒ–
        
        # === å‡å€¼æ›´æ–° ===
        y_k = [(x - self.mean)/self.sigma for x in valid_samples]
        y_w = sum(w * y_k[i] for w, i in zip(actual_weights, sorted_indices))
        self.mean += self.sigma * y_w
        
        # è¿›åŒ–è·¯å¾„æ›´æ–°ï¼ˆä¿ç•™ä½†ä¸å½±å“åæ–¹å·®çŸ©é˜µï¼‰
        generation = len(self.sigma_history)  
        p_sigma_norm = np.linalg.norm(self.p_sigma)
        threshold = (1.4 + 2/(self.dim+1)) * self.chi_n
        h_sigma = 1 if p_sigma_norm / np.sqrt(1 - (1-self.cs)**(2*(generation+1))) < threshold else 0
        
        self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs*(2-self.cs)*self.mueff) * y_w
        self.p_c = (1 - self.cc) * self.p_c + h_sigma * np.sqrt(self.cc*(2-self.cc)*self.mueff) * y_w
        
        # æ­¥é•¿æ›´æ–°ï¼ˆä¿ç•™ï¼‰
        sigma_norm = np.linalg.norm(self.p_sigma) / self.chi_n
        self.sigma *= np.exp((sigma_norm - 1) * self.cs / self.damps)
        # æ³¨é‡Šæ‰æ‰€æœ‰åæ–¹å·®çŸ©é˜µæ›´æ–°éƒ¨åˆ† --------------------------------------------------
        rank1_update = self.c1 * np.outer(self.p_c, self.p_c)  # ç§©1æ›´æ–°ï¼ˆæ³¨é‡Šï¼‰
        rank_mu_update = np.zeros_like(self.C)  # ç§©Î¼æ›´æ–°ï¼ˆæ³¨é‡Šï¼‰
        for w, i in zip(self.weights, sorted_indices):
            y = y_k[i]
            rank_mu_update += w * np.outer(y, y)
        rank_mu_update = self.cmu * rank_mu_update
        sum_weights = sum(self.weights) 
        self.C = (1 - self.c1 - self.cmu*sum_weights) * self.C + rank1_update + rank_mu_update  # ç»„åˆæ›´æ–°ï¼ˆæ³¨é‡Šï¼‰
        
        # æ³¨é‡Šæ•°å€¼ç¨³å®šæ€§å¤„ç†ï¼ˆå› åæ–¹å·®çŸ©é˜µå›ºå®šï¼‰
        self.C = (self.C + self.C.T) / 2  # ï¼ˆæ³¨é‡Šï¼‰
        self.C = np.clip(self.C, 1e-8, None)  # ï¼ˆæ³¨é‡Šï¼‰
        
        # å‡å€¼æ›´æ–°ï¼ˆä¿ç•™ï¼‰
        self.mean = self.mean.copy()
        
        # æ›´æ–°å†å²è®°å½•å¹¶æ‰“å°å‚æ•°
        self.sigma_history.append(self.sigma)
        # print(f"sigma: {self.sigma}")
        
        self.c1_history.append(self.c1)
        # print(f"c1: {self.c1}")
        
        self.cmu_history.append(self.cmu)
        # print(f"cmu: {self.cmu}")
        
        self.p_sigma_history.append(self.p_sigma.copy())
        # print(f"p_sigma: {self.p_sigma}")
        
        self.p_c_history.append(self.p_c.copy())
        # print(f"p_c: {self.p_c}")
        
        self.C_history.append(self.C.copy())
        # print(f"C: {self.C}")



class EarlyStopping:
    def __init__(self, patience=5, min_delta=0.0001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, val_acc):
        score = val_acc
        if self.best_score is None:
            self.best_score = score
            # print(f"Initial best score: {self.best_score}")
        elif score > self.best_score - self.min_delta:# å‡†ç¡®ç‡è¶Šä½è¶Šå¥½
            self.counter += 1
            # print(f"Score {score} is less than best score {self.best_score} + min_delta {self.min_delta}. Counter: {self.counter}")
            if self.counter >= self.patience:
                self.early_stop = True
                print(f"Early stopping triggered at score {score} after {self.counter} iterations without improvement.")
        else:
            self.best_score = score
            self.counter = 0
            # print(f"New best score: {self.best_score}")
        return self.early_stop
# ---------------------------- æ”»å‡»ç±» ---------------------------
class SZOAttack:
    """ç¨€ç–åŒºåŸŸä¼˜åŒ–çš„å¯¹æŠ—æ”»å‡»"""
    def __init__(self, model, image, label, block_size, device, epsilon=0.1,universal_perturbation = None,original_path=None):  # æ–°å¢epsilonå‚æ•°
        self.device = image.device  # ç»§æ‰¿è¾“å…¥å›¾åƒçš„è®¾å¤‡
        self.model = model.to(self.device)
        self.mu = block_mu
        self.original_image = image.clone().detach().to(self.device) #ç”±äºåœ¨transformå‡½æ•°ä¸­å·²ç»åšè¿‡å½’ä¸€åŒ–ï¼Œè¿™é‡Œä¹Ÿæ˜¯å½’ä¸€åŒ–è¿‡çš„
        self.label = label.to(self.device) if isinstance(label, torch.Tensor) else torch.tensor(label, device=self.device)
        #20250308 è¿™ä¸€ç‰ˆä¸çŸ¥é“æ€ä¹ˆäº†ï¼Œä¹‹å‰åæ–¹å·®çŸ©é˜µä¹Ÿèƒ½æ›´æ–°ä¸”ä¸ç”¨è¾“å‡ºæ”»å‡»æŠ¥å‘Šçš„æ—¶å€™ï¼Œlabelä¹Ÿä¸æ˜¯å¼ é‡(æˆ‘è®°å¾—æ˜¯int)ï¼Œä¹Ÿå¯ä»¥to.device(),
        #ä½†æ˜¯ç°åœ¨ä¸è¡Œäº†ï¼Œæ‰€ä»¥åªèƒ½å¼„æˆtensor.åç»­åˆå‡ºç°self.universal_perturbation å’Œ delta ä¸åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Šï¼Œdeltaä¹Ÿè¦to.device().
       # === æ–°å¢æ‰¹é‡å›¾åƒæ”¯æŒ ===
        # self.original_images_batch = images_batch.clone().detach().to(self.device)  # å½¢çŠ¶ [B, C, H, W]
        # self.labels = labels.to(self.device) if isinstance(labels, torch.Tensor) else torch.tensor(labels, device=self.device)
        # self.original_paths = original_paths  # æ–°å¢è·¯å¾„åˆ—è¡¨æ”¯æŒ

        self.deltas = []
        self.block_size = block_size
        self.epsilon = epsilon  # å…³é”®ï¼šå®šä¹‰epsilonå±æ€§
        self.original_path = original_path  # å­˜å‚¨åŸå§‹å›¾åƒè·¯å¾„
        
        # ç¡®ä¿æ‰€æœ‰åˆå§‹åŒ–å¼ é‡åœ¨GPU
        self.H, self.W = image.shape[-2:]  # ä½¿ç”¨æœ€åä¸¤ä¸ªç»´åº¦ï¼ˆH,Wï¼‰
        self.num_blocks_h = (self.H + block_size - 1) // block_size
        self.num_blocks_w = (self.W + block_size - 1) // block_size
        self.total_blocks = self.num_blocks_h * self.num_blocks_w
        
        # ä½¿ç”¨GPUå¼ é‡åˆå§‹åŒ–æ¦‚ç‡
        # ä¸‹é¢è¿™ä¸€è¡Œæ˜¯Thompson samplingä¸“é—¨åŠ çš„
        self.ts_updater = ThompsonSamplingUpdater(self.total_blocks, device)

        self.prob = torch.ones(self.total_blocks, device=self.device) / self.total_blocks
        
        # é¢„åˆ†é…GPUæ˜¾å­˜ç©ºé—´
        self.best_adv = torch.empty_like(self.original_image, device=self.device)
        self.best_loss = -np.inf
        self.target_class = None
        
        # å…¶ä»–åˆå§‹åŒ–ä»£ç ...
        self.global_acc_history = []  # æ–°å¢ï¼šåˆå§‹åŒ–å†å²è®°å½•åˆ—è¡¨
        self.eval_interval = 1        # æ¯æ¬¡è¿­ä»£è¯„ä¼°ä¸€æ¬¡å…¨å±€å‡†ç¡®ç‡

        # è¿™ä¸ªå®šä¹‰ä¹‹å‰æ²¡å†™ä¹Ÿå¯ä»¥è¿è¡Œï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼Œè¿˜æ˜¯å…ˆè¡¥ä¸Šäº†
        self.cma_blocks = {bid: CMAESBlock(block_size) for bid in range(self.total_blocks)}


        # for bid in range(self.total_blocks):
        #     block_pixels = block_size**2 * 3  # è®¡ç®—å—åƒç´ æ•° (RGB)
        #     self.cma_blocks[bid] = CMAESBlock(block_pixels)
        #     print(f"å— {bid}: åƒç´ æ•°={block_pixels}, lambda={self.cma_blocks[bid].lambda_}, mu={self.cma_blocks[bid].mu}")
        # åˆå§‹åŒ–æ—¶,æ¥æ”¶å¤–éƒ¨ä¼ å…¥çš„å…¨å±€æ‰°åŠ¨
        if universal_perturbation is None:
            self.universal_perturbation = torch.zeros_like(image, device=image.device)
            # self.universal_perturbation = torch.zeros_like(image.unsqueeze(0), device=device)

        else:
            self.universal_perturbation = universal_perturbation.squeeze(0).to(self.device)
        ##ä¸Šé¢è¿™è¡Œä»£ç çš„åŸå› : [1,3,H,W],è®¾ç½®ä¸º4ç»´,è¿™æ ·æ‰èƒ½ä¸imageåŒ¹é…(images.shape = [64, 3, 64, 64]  # Batch size=64)
        self.early_stopping = EarlyStopping(patience=10, min_delta=0.0001)  # åˆå§‹åŒ–æ—©åœç­–ç•¥

        # æ¡ä»¶åŒ–åˆå§‹åŒ–å¯è§†åŒ–ç»„ä»¶
        if debug_mode:
            self.visualizer = AttackVisualizer(
                H=self.H,  # æ­£ç¡®ä¼ é€’å‚æ•°
                W=self.W,
                block_size=self.block_size,
                device=self.device
            )
            # self.logger = CMAESLogger(
            #     total_blocks=self.total_blocks,  # å‚æ•°åä¿®æ­£log_block_selection
            #     device=self.device
            # )
        else:
            self.visualizer = None
            # self.logger = None

    def block_projection(self, delta, bid): #return delta
            """å…¨GPUåŒ–çš„å—æŠ•å½±"""
            num_blocks_per_row = (self.W + self.block_size - 1) // self.block_size
            h_start = (bid // num_blocks_per_row) * self.block_size
            w_start = (bid % num_blocks_per_row) * self.block_size
            
            # ä½¿ç”¨GPUåŠ é€Ÿçš„åˆ‡ç‰‡æ“ä½œ
            h_slice = slice(max(0, h_start), min(h_start+self.block_size, self.H))
            w_slice = slice(max(0, w_start), min(w_start+self.block_size, self.W))

            h_pixels = h_slice.stop - h_slice.start
            w_pixels = w_slice.stop - w_slice.start
            # print(f"h_pixels: ", h_pixels) #æ‰“å°å‡ºæ¥æ˜¯block_size
            # print(f"w_pixels: ", w_pixels) #æ‰“å°å‡ºæ¥ä¹Ÿæ˜¯block_size

            # è·å–å½“å‰å—çš„CMAå‚æ•°
            cma_block = self.cma_blocks[bid]
            
            # ç”Ÿæˆå…¨æ‰°åŠ¨å‘é‡ï¼ˆæœªç¨€ç–åŒ–ï¼‰
            full_perturbation = cma_block.sigma * np.random.multivariate_normal(
                cma_block.mean, cma_block.C
            )
            # print("full_perturbation.shape",full_perturbation.shape)
            
            # (ç¡¬ç¨€ç–)ç¨€ç–åŒ–ï¼šé€‰æ‹©å½±å“åŠ›æœ€å¤§çš„å‰kä¸ªåƒç´ 
            # perturbation_flat = full_perturbation.reshape(-1)
            k = int(cma_block.sparsity * len(full_perturbation))  # æ‰°åŠ¨åƒç´ æ•°é‡
            top_indices = np.argsort(np.abs(full_perturbation))[-k:]  # é€‰æ‹©ç»å¯¹å€¼æœ€å¤§çš„kä¸ª
            
            # ç”Ÿæˆç¨€ç–æ‰°åŠ¨çŸ©é˜µ
            sparse_perturbation = np.zeros(3 * h_pixels * w_pixels)  
            sparse_perturbation[top_indices] = full_perturbation[top_indices]
            sparse_perturbation = sparse_perturbation.reshape(3, self.block_size, self.block_size)  # æ­£ç¡®3Då½¢çŠ¶
    
            # è½¬æ¢ä¸ºTensorå¹¶é€‚é…æ‰¹æ¬¡ç»´åº¦
            sparse_tensor = torch.tensor(sparse_perturbation, device=self.device).unsqueeze(0)
            # print("sparse_tensor.shape:",sparse_tensor.shape)
            # å…³é”®ä¿®å¤ï¼šæ­£ç¡®åˆ‡ç‰‡å’Œç»´åº¦å¯¹é½
            # deltaå½¢çŠ¶: [batch_size, C, H, W]
            delta[:, :, h_slice, w_slice] += sparse_tensor 
            # cma_block.append(sparse_perturbation.copy())  # è¿™è¡Œå‹æ ¹å°±æ²¡æœ‰æ„ä¹‰ï¼Œç•™åœ¨è¿™é‡Œä¸ºäº†æé†’å…¶ä»–ä»£ç ä¹Ÿåˆ é™¤
            
            return delta

    def sample_perturbations(self):
        """ç”Ÿæˆåˆ†å—æ‰°åŠ¨ï¼ˆä¿®å¤num_samplesä½œç”¨ï¼‰"""
        # 1. é€‰æ‹©å¤šä¸ªå—ï¼ˆæ¯è½®é€‰num_samplesä¸ªä¸åŒçš„å—ï¼‰
        selected_blocks = torch.multinomial(self.prob, num_samples, replacement=False).cpu().numpy()
        # print("selected_blocks.shape",selected_blocks.shape)
        
        # 2. åˆå§‹åŒ–æ‰°åŠ¨å¼ é‡ï¼ˆå½¢çŠ¶ [num_samples, C* H* W]ï¼‰
        deltas = torch.zeros((num_samples, *self.original_image.shape), device=self.device)
        # print("åˆå§‹åŒ–ç”Ÿæˆçš„æ‰°åŠ¨å½¢çŠ¶:", deltas.shape) #torch.Size([5, 3, 64, 64]) 
        # 3. ä½¿ç”¨block_projectionä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆå¯¹åº”å—æ‰°åŠ¨
        for sample_idx in range(num_samples):
            bid = int(selected_blocks[sample_idx])  # è½¬æ¢ä¸ºPythonæ•´æ•°
            
            # åˆ›å»ºå½“å‰æ ·æœ¬çš„deltaå‰¯æœ¬
            single_delta = deltas[sample_idx].unsqueeze(0)  # [1, 3, H, W]
            # print("single_delta.shape",single_delta.shape)
            # åº”ç”¨å—æŠ•å½±
            perturbed = self.block_projection(single_delta, bid)
            # print("perturbed.shape:",perturbed.shape)
            # å†™å›ç»“æœ
            deltas[sample_idx] = perturbed.squeeze(0)
        
        return deltas, torch.tensor(selected_blocks, device=self.device)
        
    def evaluate(self, adv_images): # return losses
        """è¯„ä¼°å¯¹æŠ—æ ·æœ¬"""
            # å¼ºåˆ¶è¾“å…¥ä¸º4D
        # if adv_images.dim() == 3:
        #     adv_images = adv_images.unsqueeze(0)
        # elif adv_images.dim() != 4:
        #     raise ValueError(f"è¯„ä¼°è¾“å…¥å¿…é¡»ä¸º3Dæˆ–4Dï¼Œå®é™…ç»´åº¦: {adv_images.shape}")
        """æ‰¹é‡è¯„ä¼°å¤šä¸ªå€™é€‰æ ·æœ¬"""
        if adv_images.dim() not in [3, 4]:
            raise ValueError(f"è¾“å…¥ç»´åº¦é”™è¯¯: {adv_images.shape}")

        with torch.no_grad():
            logits = self.model(adv_images)
            
            if self.target_class is None:
                # éç›®æ ‡æ”»å‡»ï¼šæœ€å¤§åŒ–åŸå§‹ç±»åˆ«æŸå¤±
                target = torch.full((logits.size(0),), self.label, device=self.device)
                losses = nn.CrossEntropyLoss(reduction='none')(logits, target)
            else:
                 # ç›®æ ‡æ”»å‡»ï¼šæœ€å°åŒ–ç›®æ ‡ç±»åˆ«æŸå¤±
                target = torch.full((logits.size(0),), self.target_class, device=self.device)
                losses = -nn.CrossEntropyLoss(reduction='none')(logits, target) #å–è´Ÿå·è½¬ä¸ºæœ€å¤§åŒ–é—®é¢˜
            # æ·»åŠ æ­£åˆ™åŒ–é¡¹ï¼ˆç¡®ä¿ç³»æ•°åˆç†ï¼‰
            sparsity_penalty = torch.norm(adv_images - self.original_image, p=1)
            losses -= lambda_sparsity * sparsity_penalty 
            
            # update_prob# æ‰“å°è°ƒè¯•ä¿¡æ¯
            # print(f"å¹³å‡æŸå¤±: {losses.mecma_blocksan().item():.4f}, æ­£åˆ™åŒ–é¡¹: {sparsity_penalty.item():.4f}")
            return losses #ä¸€ä¸ªå¼ é‡ï¼ŒåŒ…å«äº†æ¯ä¸ªè¾“å…¥æ ·æœ¬çš„æŸå¤±å€¼

    def update_prob(self, blocks, fitness_values):
        """æ›´æ–°å—é€‰æ‹©æ¦‚ç‡å¹¶æ‰§è¡ŒCMA-ESå‚æ•°æ›´æ–°
        
        å‚æ•°:
            blocks: ä¸€ç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸º(num_samples,)ï¼Œå…ƒç´ ä¸ºæ¯ä¸ªæ ·æœ¬é€‰æ‹©çš„å—ID
            fitness_values: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(num_samples,)ï¼Œæ¯ä¸ªæ ·æœ¬çš„æŸå¤±å€¼
        """
            # è¾“å…¥æ ¡éªŒ
        assert len(blocks) == num_samples, "å—æ€»æ•°å¿…é¡»ç­‰äºnum_samples"
        assert len(fitness_values) == num_samples, "é€‚åº”åº¦å€¼æ•°é‡å¿…é¡»åŒ¹é…"
        # è½¬æ¢ä¸ºCPU numpyæ•°ç»„
        blocks_np = blocks.cpu().numpy() if isinstance(blocks, torch.Tensor) else blocks
        assert blocks_np.ndim == 1, "blockså¿…é¡»æ˜¯ä¸€ç»´æ•°ç»„"
        
        # 1. å®šä¹‰æˆåŠŸæ ‡å‡†ï¼ˆé€‰æ‹©å‰muä¸ªé«˜æŸå¤±æ ·æœ¬ï¼‰
        sorted_indices = np.argsort(fitness_values)[-self.mu:]
        success_blocks = blocks_np[sorted_indices]
        # æ³¨ï¼šNumPy æ•°ç»„æœ¬èº«æ˜¯ CPU-only çš„ï¼Œæ— æ³•ç›´æ¥åœ¨ GPU ä¸Šè¿è¡Œ
    
        
        # 2. æ›´æ–°Betaåˆ†å¸ƒå‚æ•°ï¼ˆThompson Samplingæ ¸å¿ƒï¼‰
        for bid in np.unique(success_blocks):
            mask = (blocks_np == bid)
            success_count = mask.sum()  # è¯¥å—è¢«é€‰ä¸ºé«˜æŸå¤±å—çš„æ¬¡æ•°
            fail_count = len(blocks) - success_count  # æœªè¢«é€‰ä¸­çš„æ¬¡æ•°
            
            # å¢é‡æ›´æ–°alpha/betaï¼ˆé¿å…è¦†ç›–å†å²ä¿¡æ¯ï¼‰
            self.ts_updater.alpha[bid] += success_count
            self.ts_updater.beta[bid] += fail_count
        
        # 3. ä»Betaåˆ†å¸ƒé‡‡æ ·æ–°æ¦‚ç‡ï¼ˆæ›¿ä»£åŸæœ‰æƒé‡è®¡ç®—ï¼‰
        self.prob = self.ts_updater.sample_probs()
        
        # 4. CMA-ESæ›´æ–°ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
        for bid in np.unique(blocks_np):
            mask = (blocks_np == bid)
            block_samples = self.cma_blocks[bid].sample()
            block_fitness = fitness_values[mask]
            
            if len(block_samples) > 0:
                samples_array = np.array(block_samples)
                if samples_array.ndim == 1:
                    samples_array = samples_array.reshape(1, -1)
                    
                self.cma_blocks[bid].update(
                    all_samples=samples_array,
                    fitness_values=block_fitness
                )
        
        # 5. åŠ¨æ€è°ƒæ•´ç¨€ç–åº¦ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
        global_best = np.max(fitness_values)
        for bid in np.unique(blocks_np):
            cma = self.cma_blocks[bid]
            if cma.best_fitness < global_best:
                cma.sparsity = min(cma.sparsity * 1.1, 0.5)
            else:
                cma.sparsity = max(cma.sparsity * 0.9, 0.05)



    def attack_batch(self, images, labels):
        """å…¨GPUæ‰¹é‡æ”»å‡»"""
        # ç¡®ä¿è¾“å…¥æ•°æ®åœ¨GPU
        images = images.to(self.device)
        labels = labels.to(self.device)
        
        # é¢„åˆ†é…ç»“æœæ˜¾å­˜
        adv_images = torch.empty_like(images, device=self.device)
        
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¤šçº¿ç¨‹
        for i in range(images.shape[0]):
            # attacker = SZOAttack(self.model, images[i], labels[i], self.block_size,device=self.device,epsilon=self.epsilon) # ä¼ é€’å…¨å±€å‚æ•°)
            # åº”ç”¨å½“å‰çš„å…¨å±€æ‰°åŠ¨
            # perturbed_img = torch.clamp(images[i] + self.universal_perturbation, 0, 1)
            perturbed_img = images[i] + self.universal_perturbation

            adv_images[i] = perturbed_img.squeeze(0)  # ç§»é™¤æ‰¹æ¬¡ç»´åº¦
            
            # æ˜¾å­˜ä¼˜åŒ–
            if i % 10 == 0:
                torch.cuda.empty_cache()
                
        return adv_images

    def attack(self, val_loader, device, enable_eval=True, show_progress=True):
        """æœ€æ ¸å¿ƒçš„å‡½æ•°ï¼šå…¨GPUåŒ–æ”»å‡»æµç¨‹ (æ–°å¢å…¨å±€å‡†ç¡®ç‡è®°å½•åŠŸèƒ½)"""
        progress_bar = tqdm(range(max_iter), desc="ğŸ”¥GPUæ”»å‡»è¿›ç¨‹") if show_progress else range(max_iter)
        
        for t in progress_bar:
            #         # === å…³é”®ä¿®å¤ï¼šæ¸…ç©ºæ‰€æœ‰å—çš„æ—§æ ·æœ¬ ===
            # for bid in self.cma_blocks.values():
            #     bid.samples = []
            # 1. ç”Ÿæˆæ‰¹é‡æ‰°åŠ¨æ ·æœ¬ (å½¢çŠ¶: [num_samples, C, H, W])å¹¶æ›´æ–°é€šç”¨æ‰°åŠ¨ åˆ†å—æ‰°åŠ¨
            deltas, blocks = self.sample_perturbations()
            # print("deltas.shape : ", deltas.shape)
            # print("blocks.shape : ",blocks.shape)
            # è¿™ä¸ªå‡½æ•°çš„å¤§ä½“ä»‹ç»ï¼šå…ˆé€‰å—å·ï¼Œç„¶åå¯¹åº”å—ç”Ÿæˆé«˜æ–¯å™ªå£°æ‰°åŠ¨
            # delta = delta.squeeze(0)  # ä» [1, C, H, W] -> [C, H, W]
            # delta = delta.unsqueeze(0).to(self./device)  # ç¡®ä¿ delta åœ¨ GPU ä¸Š
            # delta : å½¢çŠ¶:[num_samples, C, H, W]; blocksæ˜¯å°±æ˜¯selected_blocks,å½“å‰çš„çŠ¶æ€æ˜¯numpyç±»å‹ã€‚




            # print(f"\nIteration {t}, Epsilon: {self.epsilon}, Perturbation Norm: {torch.norm(self.universal_perturbation).item()}")

            # 3. ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
            candidate_advs = self.original_image.unsqueeze(0) + deltas  # 3Då¼ é‡
            # print("attack:adv_images", adv_images.shape)

            # 4. è°ƒç”¨ evaluate å‡½æ•°è®¡ç®—æŸå¤±å€¼ 
            losses = self.evaluate(candidate_advs) 
            # print("losses.shape:",losses.shape)
            best_idx = torch.argmax(losses)  # é€‰æ‹©æŸå¤±æœ€å¤§çš„å€™é€‰
            fitness_values = losses.cpu().numpy()  # é€‚åº”åº¦å€¼ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰ï¼Œè¿™é‡Œæ˜¯å°†PyTorchå¼ é‡è½¬æ¢ä¸ºNumPyæ•°ç»„
            # 2.æ›´æ–°å…¨å±€æ‰°åŠ¨
            self.universal_perturbation = torch.clamp(
                self.universal_perturbation + deltas[best_idx],
                -self.epsilon, 
                self.epsilon
            )
                    # === å…³é”®ä¿®å¤ç‚¹ï¼šå®šä¹‰å½“å‰æœ€ä½³å¯¹æŠ—æ ·æœ¬ ===
            best_adv = candidate_advs[best_idx]  # [C, H, W]
            current_adv_images = best_adv  # [C, H, W]
            if debug_mode:
                # === æ–°å¢å¯è§†åŒ–è°ƒç”¨ç‚¹1ï¼šæ‰°åŠ¨ç”Ÿæˆå ===
                self.visualizer.plot_perturbation(
                    cid=int(self.label.item()),
                    delta=deltas[best_idx].unsqueeze(0).detach().cpu(),
                    universal_pert=self.universal_perturbation.detach().cpu(),
                    iteration=t
                )
                # ==== æ–°å¢è°ƒè¯•ä»£ç  ====
                if epsilon == 0:
                    assert torch.allclose(self.original_image, current_adv_images, atol=1e-6), \
                        f"é›¶æ‰°åŠ¨ä¸ä¸€è‡´ï¼å·®å¼‚: {torch.max(torch.abs(self.original_image - current_adv_images)).item()}"
            
            #6 æ›´æ–°å—é€‰æ‹©æ¦‚ç‡
            # blocks,fitness_values:numpy_array 
            #ä¼ è¿›æ¥çš„åº”è¯¥æ˜¯åˆ†è¿‡å—çš„blocks(å¼ é‡å½¢å¼)å’Œå¯¹åº”çš„fitness_values
            self.update_prob(blocks, fitness_values) #å…¨ä»£ç ä¸­å”¯ä¸€ä¸€æ¬¡å¯¹è¿™ä¸ªå‡½æ•°çš„è°ƒç”¨
            

            if debug_mode:
                # === æ–°å¢å¯è§†åŒ–è°ƒç”¨ç‚¹2,3ï¼šæ¦‚ç‡æ›´æ–°å ===
                self.visualizer.log_block_heatmap(
                    cid=int(self.label.item()), #  1. éç›®æ ‡æ”»å‡»æ—¶ï¼Œè®°å½•è¢«æ”»å‡»çš„åŸå§‹ç±»åˆ« 2. ç›®æ ‡æ”»å‡»æ—¶ï¼Œéœ€æ”¹ä¸ºç›®æ ‡ç±»åˆ«.æ³¨ï¼šå½“ label æ˜¯å•ä¸ªæ•°å€¼å¼ é‡æ—¶ï¼ˆå¦‚åˆ†ç±»æ”»å‡»çš„ç›®æ ‡ç±»åˆ«ï¼‰ï¼Œè¿™æ˜¯æ­£ç¡®çš„æ–¹æ³•ã€‚åœ¨æ‰¹é‡æ”»å‡»æ—¶ä¼šå‡ºé”™ï¼
                    iteration=t,
                    selected_blocks=blocks.cpu().numpy().astype(np.int64)
                    )
                self.visualizer.log_block_distribution(
                    class_id=int(self.label.item()), #  1. éç›®æ ‡æ”»å‡»æ—¶ï¼Œè®°å½•è¢«æ”»å‡»çš„åŸå§‹ç±»åˆ« 2. ç›®æ ‡æ”»å‡»æ—¶ï¼Œéœ€æ”¹ä¸ºç›®æ ‡ç±»åˆ«.æ³¨ï¼šå½“ label æ˜¯å•ä¸ªæ•°å€¼å¼ é‡æ—¶ï¼ˆå¦‚åˆ†ç±»æ”»å‡»çš„ç›®æ ‡ç±»åˆ«ï¼‰ï¼Œè¿™æ˜¯æ­£ç¡®çš„æ–¹æ³•ã€‚åœ¨æ‰¹é‡æ”»å‡»æ—¶ä¼šå‡ºé”™ï¼
                    iteration=t,
                    prob_dist=self.prob.float().cpu()
                )
            
                # === æ–°å¢å¯è§†åŒ–è°ƒç”¨ç‚¹4ï¼šCMA-ESæ›´æ–° ===
                for bid in blocks.unique().cpu().numpy():
                    self.visualizer.visualize_cma_es(
                        cid=self.label.item(),
                        bid=bid,
                        cma_state={
                            'c1_history': self.cma_blocks[bid].c1_history,
                            'cmu_history': self.cma_blocks[bid].cmu_history,
                            'p_sigma_history': self.cma_blocks[bid].p_sigma_history,
                            'p_c_history': self.cma_blocks[bid].p_c_history,
                            'C_history': self.cma_blocks[bid].C_history,
                            'sigma_history': self.cma_blocks[bid].sigma_history
                        },
                        iteration=t
                    )
            # 7. è®¡ç®—å½“å‰æœ€ä½³æŸå¤±å€¼
            # === æ€§èƒ½ç›‘æ§ ===
            with torch.no_grad():
                current_loss = losses[best_idx]  # ç›´æ¥ä½¿ç”¨å·²è®¡ç®—ç»“æœ
                
            # æ›´æ–°æœ€ä½³ç»“æœï¼ˆåˆ é™¤é‡å¤è®¡ç®—ï¼‰
            if current_loss > self.best_loss:
                self.best_loss = current_loss
                self.best_adv = current_adv_images.detach().clone()
            
            # 9. è®°å½•å…¨å±€å‡†ç¡®ç‡
            if enable_eval and (t % self.eval_interval == 0):
                attacked_val_images = []
                attacked_val_labels = []
                for images, labels in val_loader:
                    images = images.to(self.device)
                    labels = labels.to(self.device)
                    adv_images = self.attack_batch(images, labels)
                    attacked_val_images.append(adv_images)
                    attacked_val_labels.append(labels)
                    # ------------------è°ƒè¯•begin: é€ä¸ªå›¾åƒå¤„ç†(è°ƒè¯•ç»“æœ:ä¸ºæ”»å‡»æƒ…å†µä¸‹å‡†ç¡®ç‡è¿‡ä½çš„åŸå› ä¸åœ¨è¿™)----------------
                    # for i in range(images.shape[0]):
                    #     perturbed_img = torch.clamp(images[i] + self.universal_perturbation, 0, 1)  # æ·»åŠ å…¨å±€æ‰°åŠ¨
                    #     attacked_val_images.append(perturbed_img)  # æ·»åŠ æ‰°åŠ¨åçš„å›¾åƒï¼Œå¹¶ä¿æŒæ‰¹æ¬¡ç»´åº¦
                    #     attacked_val_labels.append(labels[i].unsqueeze(0))  # æ·»åŠ æ ‡ç­¾ï¼Œå¹¶ä¿æŒæ‰¹æ¬¡ç»´åº¦
                    # ------------------è°ƒè¯•end: é€ä¸ªå›¾åƒå¤„ç†----------------

                attacked_val_images = torch.cat(attacked_val_images, dim=0)
                attacked_val_labels = torch.cat(attacked_val_labels, dim=0)

                attacked_dataset = TensorDataset(attacked_val_images, attacked_val_labels)
                attacked_dataloader = DataLoader(attacked_dataset, batch_size=64, shuffle=False)
                current_acc = evaluate_global_accuracy(self.model, attacked_dataloader, device)
                self.global_acc_history.append(current_acc)
                
                if current_acc > 0 and self.early_stopping(current_acc):
                    print(f"Early stopping at iteration {t} due to no improvement in global accuracy.")
                    break

            global_acc = np.mean(self.global_acc_history)

            # 8. æ›´æ–°è¿›åº¦æ¡
            if enable_eval:
                progress_bar.set_postfix(
                    Global_Acc=f"{global_acc:.2f}%" if enable_eval else "N/A", 
                    Current_Acc=f"{current_acc:.2f}%", 
                    Loss=f"{self.best_loss:.4f}"
                )
        
        # ç¡®ä¿ global_acc_history é•¿åº¦è¶³å¤Ÿ
        if len(self.global_acc_history) < max_iter:
            self.global_acc_history.extend([self.global_acc_history[-1]] * (max_iter - len(self.global_acc_history)))
        
        return self.best_adv.squeeze(0)

    def save_attack_report(self, orig_img, adv_img, true_label, adv_label, original_path, iteration):
            """ä¿å­˜æ”»å‡»æ•ˆæœæŠ¥å‘Šå›¾ï¼ˆæŒ‰ç±»åˆ«ç»„ç»‡ç›®å½•ç»“æ„ï¼‰"""
            # è§£æåŸå§‹è·¯å¾„è·å–æ•°æ®é›†ç±»å‹å’Œç±»åˆ«ä¿¡æ¯
            path_parts = original_path.split(os.sep)
            
            # å®šä½å…³é”®è·¯å¾„èŠ‚ç‚¹ï¼ˆå‡è®¾è·¯å¾„ç»“æ„ä¸º: .../tiny-imagenet-200/[trainæˆ–val]/class_folder/images/xxx.JPEGï¼‰
            try:
                dataset_type = path_parts[path_parts.index('tiny-imagenet-200') + 1]  # è·å–trainæˆ–val
                class_folder = path_parts[path_parts.index(dataset_type) + 1]        # è·å–ç±»åˆ«æ–‡ä»¶å¤¹åç§°
            except ValueError:
                dataset_type = "unknown_dataset"
                class_folder = "unknown_class"

            # æ„å»ºä¿å­˜è·¯å¾„ï¼ˆæ ¼å¼: attacked_images/[train|val]/[class_folder]/ï¼‰
            base_path = os.path.join("attacked_images", dataset_type, class_folder)
            os.makedirs(base_path, exist_ok=True)

            # ç”Ÿæˆå¸¦è¿­ä»£æ¬¡æ•°çš„æ–‡ä»¶å
            filename = os.path.basename(original_path).split('.')[0]
            save_path = os.path.join(base_path, f"{filename}_iter{iteration}_report.png")

            # ä»¥ä¸‹ç»˜å›¾ä»£ç ä¿æŒä¸å˜...
            orig_img_np = orig_img.cpu().numpy().transpose(1, 2, 0)
            adv_img_np = adv_img.cpu().numpy().transpose(1, 2, 0)
            
            # è®¡ç®—å™ªå£°ï¼ˆåŸºäºå½’ä¸€åŒ–åçš„å¼ é‡ï¼‰
            noise = (adv_img - orig_img).abs().sum(0, keepdim=True)
            noise_norm = noise.squeeze().cpu().numpy()
            noise_norm = (noise_norm - noise_norm.min()) / (noise_norm.max() - noise_norm.min() + 1e-8)

            # ç»˜åˆ¶ä¸‰å›¾åˆä¸€æŠ¥å‘Š
            plt.figure(figsize=(15, 5))
            
            # åŸå§‹å›¾åƒï¼ˆå½’ä¸€åŒ–åï¼‰
            plt.subplot(1, 3, 1)
            plt.imshow(orig_img_np)
            plt.title(f"Original\nLabel: {true_label}")
            plt.axis('off')

            # å¯¹æŠ—æ ·æœ¬ï¼ˆå½’ä¸€åŒ–åï¼Œæœªè£å‰ªï¼‰
            plt.subplot(1, 3, 2)
            plt.imshow(adv_img_np)
            plt.title(f"Adversarial\nPred: {adv_label}")
            plt.axis('off')

            # å™ªå£°çƒ­åŠ›å›¾
            plt.subplot(1, 3, 3)
            heatmap = plt.imshow(noise_norm, cmap='viridis_r', vmin=0, vmax=0.3)
            plt.colorbar(heatmap, fraction=0.046, pad=0.04)
            plt.title("Perturbation Heatmap")
            plt.axis('off')

            plt.tight_layout()
            plt.savefig(save_path, bbox_inches='tight')
            plt.close()
class AttackVisualizer:
    """ä¸SZOAttackå°ºå¯¸å®Œå…¨å…¼å®¹çš„å¯è§†åŒ–ç³»ç»Ÿ"""
    def __init__(self, H, W, block_size, device):
        self.H = H
        self.W = W
        self.block_size = block_size
        self.device = device
        self._init_filesystem()
        
    def _init_filesystem(self):
        """åˆ›å»ºå®Œæ•´çš„æ—¥å¿—ç›®å½•ä½“ç³»,åœ¨ç±»åˆå§‹åŒ–é˜¶æ®µä½¿ç”¨"""
        # å…¨å±€ç›®å½•
        os.makedirs("attack_logs/perturbations", exist_ok=True)
        
        # ç±»ä¸“å±ç›®å½•ï¼ˆæŒ‰æœ€å¤§å¯èƒ½ç±»åˆ«æ•°é¢„åˆ›å»ºï¼‰
        for cid in range(num_test_classes):  # å‡è®¾æœ€å¤š200ä¸ªç±»åˆ«
            class_dirs = [
                f"attack_logs/class_{cid}/block_selections",
                f"attack_logs/class_{cid}/cma_es",
                f"attack_logs/perturbations/class_{cid}"
            ]
            for d in class_dirs:
                os.makedirs(d, exist_ok=True)

    def log_block_heatmap(self, cid, iteration, selected_blocks):
        """çƒ­åŠ›å›¾ä¸“ç”¨æ–¹æ³•ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªç‰ˆæœ¬æ ¸å¿ƒé€»è¾‘ï¼‰,æ­¤æ–¹æ³•åœ¨def attackä¸­è¢«ç›´æ¥è°ƒç”¨"""
        grid_size = self.H // self.block_size
        heatmap = torch.zeros((grid_size, grid_size), device='cpu')

        # è½¬æ¢å—IDåˆ°åæ ‡
        for bid in selected_blocks:
            row = bid // grid_size
            col = bid % grid_size
            heatmap[row, col] += 1

        # ç”Ÿæˆå›¾åƒ
        plt.imshow(heatmap.numpy(), cmap='viridis')
        plt.colorbar()
        plt.title(f"Class {cid} Block Heatmap\nIter {iteration}")
        plt.savefig(f"attack_logs/class_{cid}/block_selections/iter_{iteration:04d}.png")
        plt.close()

    def log_block_distribution(self, class_id, iteration, prob_dist):
        """æ¦‚ç‡åˆ†å¸ƒä¸“ç”¨æ–¹æ³•ï¼ˆæ•´åˆç¬¬äºŒä¸ªç‰ˆæœ¬ç‰¹æ€§ï¼‰,æ­¤æ–¹æ³•åœ¨def attackä¸­è¢«ç›´æ¥è°ƒç”¨"""
        plt.figure(figsize=(10, 4))
        plt.bar(range(len(prob_dist)), prob_dist.numpy(), alpha=0.7)
        plt.xlabel('Block ID')
        plt.ylabel('Selection Probability')
        plt.title(f"Class {class_id} Probability Distribution\nIter {iteration}")
        plt.savefig(f"attack_logs/class_{class_id}/block_selections/probs_iter_{iteration:04d}.png")
        plt.close()

    # def visualize_cma_es(self, cid, bid, cma_state, iteration):
    #     """ä»…æ˜¾ç¤ºCMA-ESå‚æ•°å˜åŒ–æ›²çº¿"""
    #     plt.figure(figsize=(12, 6))
        
    #     # ================== å‚æ•°æ¼”åŒ–æ›²çº¿ ==================
    #     # åˆ›å»º2x2å­å›¾å¸ƒå±€
    #     ax1 = plt.subplot(2, 2, 1)  # å­¦ä¹ ç‡å‚æ•°
    #     ax2 = plt.subplot(2, 2, 2)  # è¿›åŒ–è·¯å¾„èŒƒæ•°
    #     ax3 = plt.subplot(2, 2, 3)  # æ­¥é•¿å†å²
    #     ax4 = plt.subplot(2, 2, 4)  # åæ–¹å·®æ¡ä»¶æ•°
        
    #     # ç¡®ä¿æ‰€æœ‰å†å²è®°å½•å­˜åœ¨
    #     param_history = {
    #         'c1': cma_state.get('c1_history', []),
    #         'cmu': cma_state.get('cmu_history', []),
    #         'p_sigma': cma_state.get('p_sigma_history', []),
    #         'p_c': cma_state.get('p_c_history', []),
    #         'C': cma_state.get('C_history', [])
    #     }
        
    #     # æ›²çº¿1: å­¦ä¹ ç‡å‚æ•°
    #     ax1.plot(param_history['c1'], color='#1f77b4', label='c1')
    #     ax1.plot(param_history['cmu'], color='#ff7f0e', label='cmu')
    #     ax1.set_title("Learning Rates")
    #     ax1.set_ylabel("Value")
    #     ax1.set_ylim(0, 1.2)
    #     ax1.legend()
        
    #     # æ›²çº¿2: è¿›åŒ–è·¯å¾„èŒƒæ•°
    #     p_sigma_norms = [np.linalg.norm(p) for p in param_history['p_sigma']]
    #     p_c_norms = [np.linalg.norm(p) for p in param_history['p_c']]
    #     ax2.semilogy(p_sigma_norms, color='#2ca02c', label='||p_Ïƒ||')
    #     ax2.semilogy(p_c_norms, color='#d62728', label='||p_c||')
    #     ax2.set_title("Evolution Path Norms")
    #     ax2.set_ylabel("Norm (log scale)")
    #     ax2.legend()
        
    #     # æ›²çº¿3: æ­¥é•¿å†å²
    #     ax3.semilogy(cma_state['sigma_history'], color='#9467bd')
    #     ax3.set_title("Step Size History")
    #     ax3.set_xlabel("Iteration")
    #     ax3.set_ylabel("Ïƒ (log scale)")
        
    #     # æ›²çº¿4: åæ–¹å·®æ¡ä»¶æ•°
    #     cond_numbers = [np.linalg.cond(c) for c in param_history['C']]
    #     ax4.semilogy(cond_numbers, color='#8c564b')
    #     ax4.set_title("Covariance Condition")
    #     ax4.set_xlabel("Iteration")
    #     ax4.set_ylabel("Condition Number")
        
    #     plt.tight_layout()
    #     plt.savefig(f"attack_logs/class_{cid}/cma_es/block_{bid}.png", 
    #             dpi=150, bbox_inches='tight')
    #     plt.close()
    def visualize_cma_es(self, cid, bid, cma_state, iteration):
        """å°†CMA-ESå‚æ•°å˜åŒ–æ›²çº¿ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶"""
        # æŒ‡å®šä¿å­˜è·¯å¾„
        save_path = f"attack_logs/class_{cid}/cma_es/block_{bid}.txt"
        
        # ç¡®ä¿æ‰€æœ‰å†å²è®°å½•å­˜åœ¨
        param_history = {
            'c1': cma_state.get('c1_history', []),
            'cmu': cma_state.get('cmu_history', []),
            'p_sigma': cma_state.get('p_sigma_history', []),
            'p_c': cma_state.get('p_c_history', []),
            'C': cma_state.get('C_history', []),
            'sigma': cma_state.get('sigma_history', [])
        }
        # print(param_history)
        # å°†å†å²è®°å½•å†™å…¥æ–‡æœ¬æ–‡ä»¶
        with open(save_path, 'w') as f:
            f.write("Iteration, c1, cmu, ||p_sigma||, ||p_c||, sigma, condition number\n")
            for i, (c1, cmu, p_sigma, p_c, C, sigma) in enumerate(zip(
                    param_history['c1'], param_history['cmu'], 
                    [np.linalg.norm(p) for p in param_history['p_sigma']], 
                    [np.linalg.norm(p) for p in param_history['p_c']], 
                    [np.linalg.cond(c) for c in param_history['C']], 
                    param_history['sigma']
                )):
                f.write(f"{i}, {c1}, {cmu}, {p_sigma}, {p_c}, {sigma}, {C}\n")

        # print(f"CMA-ESå‚æ•°å†å²å·²ä¿å­˜åˆ° {save_path}")
    def plot_perturbation(self, cid, delta, universal_pert, iteration):
        """å®Œå…¨ä¿®å¤çƒ­åŠ›å›¾ç»´åº¦é—®é¢˜çš„å¯è§†åŒ–å‡½æ•°"""
        # === 1. è¾“å…¥é¢„å¤„ç† ===
        def _ensure_3d(tensor):
            tensor = tensor.squeeze()
            if tensor.dim() == 4:
                tensor = tensor[0]
            if tensor.dim() != 3:
                raise ValueError(f"è¾“å…¥åº”ä¸º3Dæˆ–4Då¼ é‡ï¼Œå®é™…å¾—åˆ°: {tensor.shape}")
            return tensor

        # === 2. å®‰å…¨å¤„ç† ===
        try:
            # å¤„ç†åŸå§‹è¾“å…¥
            delta_3d = _ensure_3d(delta)
            pert_3d = _ensure_3d(universal_pert)
            
            # === å…³é”®ä¿®å¤ï¼šæ­£ç¡®çš„2Dçƒ­åŠ›å›¾è®¡ç®— ===
            heatmap = torch.norm(pert_3d, p=2, dim=0)  # æ²¿é€šé“ç»´è®¡ç®—ï¼Œè¾“å‡º[H,W]
            
            # è½¬æ¢ä¸ºnumpyå¹¶è°ƒæ•´å¯¹æ¯”åº¦
            delta_np = delta_3d.permute(1, 2, 0).cpu().numpy()
            pert_np = pert_3d.permute(1, 2, 0).cpu().numpy()
            delta_np = np.clip(delta_np * 2.5 + 0.5, 0, 1)  # ç¼©æ”¾å¹¶å®‰å…¨è£å‰ª
            pert_np = np.clip(pert_np * 2.5 + 0.5, 0, 1)  # æ›´æ¸©å’Œçš„ç¼©æ”¾å‚æ•°
            heatmap = heatmap.cpu().numpy()

        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–é¢„å¤„ç†å¤±è´¥: {str(e)}")
            return

        # === 3. ç»˜å›¾ ===
        fig, axs = plt.subplots(1, 3, figsize=(18, 6))
        
        # å½“å‰æ‰°åŠ¨
        axs[0].imshow(delta_np)
        axs[0].set_title(f"Class {cid} Iter {iteration} Delta")
        
        # ç´¯ç§¯æ‰°åŠ¨
        axs[1].imshow(pert_np)
        axs[1].set_title(f"Class {cid} Accumulated Pert")
        
        # çƒ­åŠ›å›¾ï¼ˆç¡®ä¿2Dï¼‰
        if heatmap.ndim != 2:
            heatmap = heatmap.mean(axis=0)  # åº”æ€¥å¤„ç†ï¼šå–é€šé“å‡å€¼
        im = axs[2].imshow(heatmap, cmap='inferno')
        plt.colorbar(im, ax=axs[2])
        axs[2].set_title(f"Class {cid} Pert Magnitude")

        # === 4. ä¿å­˜ ===
        os.makedirs(f"attack_logs/perturbations/class_{cid}", exist_ok=True)
        plt.savefig(f"attack_logs/perturbations/class_{cid}/iter_{iteration:04d}.png", 
                bbox_inches='tight', dpi=150)
        plt.close()
        torch.cuda.empty_cache()

# è¿™ä¸ªå‡½æ•°è¢«æš‚æ—¶å¼ƒç”¨äº†ï¼Œå› ä¸ºå®ƒçš„å‡†ç¡®ç‡è®¡ç®—æ–¹å¼ä¸å¯¹
# åŸå› æ˜¯è¿™é‡Œé¢å¤šè¿›è¡Œäº†ä¸€æ¬¡æ ‡å‡†åŒ–ã€‚æ ‡å‡†åŒ–åªèƒ½è¿›è¡Œä¸€æ¬¡ï¼Œä¸èƒ½å¤šåšï¼ï¼ï¼
# è¿˜æœ‰ä¸€ä¸ªåŸå› ï¼šéš¾é“è®¡ç®—å‡†ç¡®ç‡åº”è¯¥ä½¿ç”¨å…¨å±€å‡½æ•°ï¼Œè€Œä¸è¦è®©modelä¼ å…¥ä¸€ä¸ªç±»å†ä½¿ç”¨ç±»å†…å‡½æ•°è®¡ç®—å—ï¼Ÿè¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿ
# ============================å·¥å…·å‡½æ•°===================
def evaluate_global_accuracy(model, val_loader, device):
    """è¯„ä¼°å¸¦/ä¸å¸¦æ‰°åŠ¨çš„å…¨å±€å‡†ç¡®ç‡,å…¶ä¸­ï¼š
    torch.no_grad() : ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œä¸»è¦ç”¨äº æ¨ç†(inference)æˆ– è¯„ä¼°æ¨¡å‹æ—¶ã€‚å®ƒèƒ½å¤Ÿ å‡å°‘å†…å­˜å ç”¨ å’Œ åŠ é€Ÿè®¡ç®—ï¼Œå› ä¸ºåœ¨è¯„ä¼°é˜¶æ®µï¼Œæˆ‘ä»¬å¹¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ã€‚
    torch.cuda.amp.autocast() : è‡ªåŠ¨æ··åˆç²¾åº¦ã€‚ä½¿ç”¨fp16åŠ é€Ÿ,é™ä½æ˜¾å­˜å ç”¨"""
    model.eval()
    correct = 0
    total = 0
    # with torch.no_grad(), torch.cuda.amp.autocast(): 
    with torch.no_grad(): #å› ä¸ºlabel 
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            # åœ¨ outputs å¼ é‡çš„ç¬¬ 1 ä¸ªç»´åº¦ï¼ˆå³ç±»åˆ«ç»´åº¦ï¼‰ä¸Šæ‰¾åˆ°æœ€å¤§å€¼ï¼Œå¹¶è¿”å›æœ€å¤§å€¼åŠå…¶å¯¹åº”çš„ç´¢å¼•ã€‚è¿™é‡Œä½¿ç”¨äº† Python çš„å…ƒç»„è§£åŒ…è¯­æ³•ï¼Œ_ è¡¨ç¤ºæˆ‘ä»¬ä¸å…³å¿ƒæœ€å¤§å€¼å…·ä½“æ˜¯å¤šå°‘ï¼Œåªå…³å¿ƒæœ€å¤§å€¼æ‰€åœ¨çš„ç´¢å¼•
            _, predicted = outputs.max(1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
    return correct / total * 100




# ---------------------------- ä¸»å‡½æ•° ---------------------------
def main():
    import warnings
    warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")
    # åˆå§‹åŒ–è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    data_root = "../tiny-imagenet-200"
    # set_seed()
    
    # æ•°æ®åŠ è½½val_loader åœ¨åç»­ä»£ç ä¸­å¤šæ¬¡è¢«ä½¿ç”¨ï¼Œä¾‹å¦‚åœ¨è¯„ä¼°æ¨¡å‹çš„åŸå§‹å‡†ç¡®ç‡å’Œè¿›è¡Œå¯¹æŠ—æ”»å‡»æ—¶ï¼Œéƒ½ä¼šä» val_loader ä¸­è·å–æ•°æ®ã€‚
    # val_loaderä¸­çš„æ•°æ®æ˜¯å·²ç»å½’ä¸€åŒ–çš„
    val_loader = get_tinyimagenet_loader(data_root, train=False) 
    
    # åŠ è½½æ¨¡å‹
    model = DebugVGG16(num_classes=200).to(device) #æ¨¡å‹ç§»åŠ¨åˆ°GPU
    # åœ¨è¿›è¡Œæ¨ç†æ—¶ï¼Œç¡®ä¿è°ƒç”¨ model.eval()ï¼
    model.eval()
    #=================debug : éªŒè¯ç‰¹å¾å›¾å°ºå¯¸(å¼€å§‹)===========
    dummy_input = torch.randn(1, 3, 64, 64).to(device)
    with torch.no_grad():
        output = model(dummy_input)
        print("è¾“å‡ºå¼ é‡å½¢çŠ¶:", output.shape)  # åº”è¾“å‡º [1, 200]
        features = model.features(dummy_input)
        print(f"ç‰¹å¾å›¾å°ºå¯¸: {features.shape}")  # åº”è¾“å‡ºå¦‚ [1, 512, 2, 2]
        flattened = torch.flatten(features, 1)
        print(f"å±•å¹³åç»´åº¦: {flattened.shape}")   # åº”å¦‚ [1, 512*2*2=2048]
    # =================debug : éªŒè¯ç‰¹å¾å›¾å°ºå¯¸(ç»“æŸ)=========== 
    # éªŒè¯åŸå§‹å‡†ç¡®ç‡
    orig_acc = evaluate_global_accuracy(model, val_loader, device) # è¿™é‡Œçš„åŸå§‹å‡†ç¡®ç‡å·²ç»æ­£å¸¸äº†,ä¾§é¢è¯´æ˜è¿™ä¸ªå‡½æ•°æ˜¯æ²¡æœ‰é—®é¢˜çš„ã€‚å¦‚æœä¸æ­£å¸¸å¯èƒ½æ˜¯å½’ä¸€åŒ–æ¬¡æ•°è¶…è¿‡ä¸€æ¬¡æˆ–è€…åšäº†ä¸å¿…è¦çš„æ•°æ®å¢å¼ºå¯¼è‡´
    
    print(f"\nåŸå§‹æ¨¡å‹å‡†ç¡®ç‡: {orig_acc:.2f}%")
    
    # æŒ‰ç±»åˆ«æŠ½æ ·æµ‹è¯•ï¼ˆä¿®æ”¹æ ‡ç­¾å¤„ç†éƒ¨åˆ†ï¼‰
    # æŠ½æ ·å¾—åˆ°çš„æ ·æœ¬å°†å­˜å‚¨åœ¨ class_samples å­—å…¸ä¸­ï¼Œimg æ˜¯è¯¥ç±»åˆ«çš„å›¾åƒå¼ é‡ï¼Œlbl æ˜¯å¯¹åº”çš„æ ‡ç­¾å¼ é‡ã€‚
    # class_sampleså­˜æ”¾çš„æ˜¯åŸå§‹çš„å›¾åƒå¼ é‡å’Œæ ‡ç­¾
    # class_samples = {}
    # dataset = val_loader.dataset
    # for idx in range(len(dataset)):
    #     img, lbl = dataset[idx]
    #     lbl_value = lbl
    #     if lbl_value not in class_samples:
    #         # å­˜å‚¨å›¾åƒè·¯å¾„ä¿¡æ¯
    #         img_pth = dataset.imgs[idx][0]
    #         label_tensor = torch.tensor(lbl, dtype=torch.long)  # è½¬æ¢ä¸ºå¼ é‡
    #         class_samples[lbl_value] = (img, label_tensor, img_pth)
    #     if len(class_samples) >= num_test_classes:
    #         break
    class_samples = {}
    dataset = val_loader.dataset

    for idx in range(len(dataset)):
        # è·å–åŸå§‹æ•°æ®é›†å’Œç´¢å¼•ï¼ˆå…¼å®¹Subsetï¼‰
        if isinstance(dataset, torch.utils.data.Subset):
            original_dataset = dataset.dataset
            original_idx = dataset.indices[idx]
        else:
            original_idx = idx
        
        # è·å–æ ·æœ¬æ•°æ®ï¼ˆä¿æŒåŸæœ‰æ–¹å¼ï¼‰
        img, lbl = dataset[idx]
        
        # è·å–å›¾åƒè·¯å¾„ï¼ˆå…¼å®¹ä¸¤ç§æ•°æ®é›†ç±»å‹ï¼‰
        img_pth = original_dataset.imgs[original_idx][0]  # å…³é”®ä¿®æ”¹ç‚¹
        
        # å­˜å‚¨é€»è¾‘ä¿æŒä¸å˜
        lbl_value = lbl.item() if isinstance(lbl, torch.Tensor) else lbl
        if lbl_value not in class_samples:
            label_tensor = torch.tensor(lbl, dtype=torch.long)
            class_samples[lbl_value] = (img, label_tensor, img_pth)
        
        if len(class_samples) >= num_test_classes:
            break
    
    # æ‰§è¡Œæ”»å‡»
    attack_success = 0
    processed_samples = 0
    total_l0, total_l1 = 0, 0.0
    # åœ¨mainå‡½æ•°ä¸­å®šä¹‰å…¨å±€æ‰°åŠ¨,æ³¨æ„è¿™ä¸ªæ‰°åŠ¨æ˜¯è·¨ç±»åˆ«çš„,è¿™æ ·åœ¨ä¸åŒç±»åˆ«ä¸Šçš„æ‰°åŠ¨æ•ˆæœæ‰èƒ½ç›¸äº’å½±å“
    universal_perturbation = torch.zeros((1,3,64,64), device=device)  # åˆå§‹åŒ–ä¸ºå››ç»´
    # è¿™ä¸ªæ•°æ®ç»“æ„:å­˜å‚¨æ‰€æœ‰ç±»åˆ«çš„æ”»å‡»å†å²
    all_class_history = {
        # æ ¼å¼ï¼š{class_id: {'global_acc': [], 'iterations': []}, ...}
    }

    test_progress = tqdm(class_samples.items(), desc="æ”»å‡»è¿›åº¦")
    
    for class_id, (image, label,img_path) in test_progress:  # æ­¤å¤„labelå·²ç»æ˜¯å¼ é‡
        # è¿™ä¸¤è¡Œæ˜¯å½“å‰ç‰ˆæœ¬æ–°åŠ çš„:å°†å½“å‰å›¾åƒå’Œæ ‡ç­¾ç§»åŠ¨åˆ°GPU
        image = image.to(device)
        label = label.to(device)
        attacker = SZOAttack(
            model=model,
            image=image, # åŸå§‹å›¾åƒ
            label=label, # åŸå§‹æ ‡ç­¾(è¿™é‡Œæ˜¯å¼ é‡)
            block_size=block_size,
            device=device,
            epsilon=epsilon,  # ä¼ é€’å…¨å±€å‚æ•°,ä¸ç„¶ä¼šä½¿ç”¨é»˜è®¤å€¼0.1
            universal_perturbation=universal_perturbation,  # ä¼ å…¥å…±äº«æ‰°åŠ¨
            original_path=img_path

        )
        # print("Visualizer exists:", hasattr(attacker, 'visualizer'))  # åº”è¾“å‡ºTrue,æ‰“å°çš„æ—¶å€™è¿™ä¸ªå¯ä»¥æ”¾å¼€ï¼Œç»ˆç«¯å¯ä»¥çœ‹åˆ°å‡†ç¡®ç‡å’Œlosså˜åŒ–è¿‡ç¨‹ã€‚
        # print("Logger exists:", hasattr(attacker, 'logger'))          # åº”è¾“å‡ºTrue
        adv_img = attacker.attack(val_loader, device) #è¿™é‡Œé¢æœ‰update_prob()->update()ï¼šcma-eså‚æ•°æ›´æ–°ã€‚
        # è®°å½•å½“å‰ç±»åˆ«çš„æ”»å‡»å†å²
        all_class_history[class_id] = {
            'global_acc': attacker.global_acc_history,
            'iterations': list(range(0, max_iter, attacker.eval_interval))
        }

        for class_id, data in all_class_history.items():
            plt.figure(figsize=(10, 5))
            plt.plot(
                data['iterations'], 
                data['global_acc'], 
                marker='o', 
                label=f'Class {class_id} Attack'
            )
            plt.axhline(y=orig_acc, color='r', linestyle='--', label='Original Accuracy')
            plt.title(f"Global Accuracy During Class {class_id} Attack")
            plt.xlabel("Attack Iteration")
            plt.ylabel("Accuracy (%)")
            plt.legend()
            plt.grid(True)
            plt.savefig(f"global_acc_class_{class_id}.png")
            plt.close()  # é¿å…å†…å­˜æ³„æ¼


        universal_perturbation = attacker.universal_perturbation  # æ›´æ–°å…¨å±€æ‰°åŠ¨
        
        # è¯„ä¼°æ”»å‡»æ•ˆæœï¼ˆä¿®æ”¹æ¯”è¾ƒé€»è¾‘ï¼‰
        with torch.no_grad():
            logits = model(adv_img.unsqueeze(0))
            pred = logits.argmax().item()
            attack_success += int(pred != label.item())  # ç”¨.item()è·å–æ•´å‹å€¼æ¯”è¾ƒ
            processed_samples += 1  # æ­£ç¡®è®¡æ•°
        # è®¡ç®—æ‰°åŠ¨æŒ‡æ ‡
        delta = (adv_img - image).abs().sum(0)
        total_l0 += (delta > 0.005).sum().item()
        total_l1 += delta.sum().item()
        
        # æ›´æ–°è¿›åº¦
        current_sr = (attack_success /processed_samples) * 100
        test_progress.set_postfix(Success=f"{current_sr:.1f}%")
    
    # æ‰“å°ç»“æœ
    print(f"\næœ€ç»ˆæ”»å‡»æˆåŠŸç‡: {(attack_success /processed_samples)*100:.1f}%")
    print(f"å¹³å‡L0æ‰°åŠ¨: {total_l0/len(class_samples):.1f}, å¹³å‡L1æ‰°åŠ¨: {total_l1/len(class_samples):.1f}")
    
    # åªåŠ è½½ä¸€ä¸ªç±»çš„ç»˜å›¾é€»è¾‘(æš‚æ—¶æ³¨é‡Šæ‰)
    # if attacker.global_acc_history:
    #     plt.figure(figsize=(10, 5))
    #     x_axis = np.arange(0, len(attacker.global_acc_history)*attacker.eval_interval, attacker.eval_interval)
    #     plt.plot(x_axis, attacker.global_acc_history, marker='o', color='red', label='Global Acc (Perturbed)')
        
    #     # ç»˜åˆ¶åŸå§‹å‡†ç¡®ç‡ä½œä¸ºå¯¹æ¯”åŸºçº¿
    #     plt.axhline(y=orig_acc, color='blue', linestyle='--', label='Original Accuracy')
        
    #     plt.title("Global Accuracy Under Universal Adversarial Attack")
    #     plt.xlabel("Attack Iteration")
    #     plt.ylabel("Accuracy (%)")
    #     plt.legend()
    #     plt.grid(True)
    #     plt.savefig("global_acc_trend.png")
    plt.figure(figsize=(12, 6))
    colors = plt.cm.viridis(np.linspace(0, 1, len(all_class_history)))

    for (class_id, data), color in zip(all_class_history.items(), colors):


        # å¡«å……æ•°æ®ï¼Œä½¿å¾— global_acc å’Œ iterations çš„é•¿åº¦ç›¸åŒ
        if len(data['global_acc']) < max_iter:
            data['global_acc'].extend([data['global_acc'][-1]] * (max_iter - len(data['global_acc'])))
        if len(data['iterations']) < max_iter:
            data['iterations'].extend([data['iterations'][-1]] * (max_iter - len(data['iterations'])))

        plt.plot(
            data['iterations'], 
            data['global_acc'], 
            marker='o', 
            color=color,
            linewidth=2,
            markersize=8,
            label=f'Class {class_id}'
        )

    plt.axhline(y=orig_acc, color='black', linestyle='--', label='Baseline')
    plt.title("Global Accuracy Under Multi-Class Attacks")
    plt.xlabel("Attack Iteration")
    plt.ylabel("Accuracy (%)")
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # å›¾ä¾‹æ”¾åœ¨å³ä¾§
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("all_classes_accuracy_trend.png", bbox_inches='tight')


    with open('attack_history.json', 'w') as f:
        json.dump(all_class_history, f)

if __name__ == "__main__":
    main()